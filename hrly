-- ─────────────────────────────────────────────────────────────
-- Build hour-level table from 15-minute slices
--   • TomTom midpoint + resample for percentiles
--   • Handles slices whose speedPercentiles = [] (sampleSize = 1)
--   • Keeps hour row even when all four slices are empty
-- ─────────────────────────────────────────────────────────────
CREATE OR REPLACE TABLE
  `stl-datascience.tomtom.tt_bulk_test_geohash6_hourly`
PARTITION BY TIMESTAMP_TRUNC(dateHour, MONTH)
CLUSTER BY geohash, dsegId, dateHour AS

WITH source AS (
  -- ==========================================================
  -- Toggle the next line for a cheap repeatable 0.1 % test run
  -- ==========================================================
  SELECT *
  FROM `stl-datascience.tomtom.tt_bulk_test_geohash6`
  -- TABLESAMPLE SYSTEM (0.1 PERCENT) REPEATABLE (42)  -- ← enable for QA
),

simulated AS (
  SELECT
    dsegId,
    geohash,
    TIMESTAMP_TRUNC(dateHour, HOUR)           AS hour_ts,
    sampleSize,
    averageSpeedMetersPerHour,
    harmonicAverageSpeedMetersPerHour,
    medianSpeedMetersPerHour,
    standardDeviationSpeedMetersPerHour,

    -- ---------- synthetic sample array (may be empty) ----------
    IF(
      ARRAY_LENGTH(speedPercentiles) = 19,
      tomtom.resampleSpeedValues(
        tomtom.midpointMethodInt(
          tomtom.arrayMetersToMiles(speedPercentiles),
          CAST(GREATEST(
            0,
            tomtom.arrayMetersToMiles(speedPercentiles)[SAFE_OFFSET(0)]
            - 1.2 * (
              tomtom.arrayMetersToMiles(speedPercentiles)[SAFE_OFFSET(1)]
              - tomtom.arrayMetersToMiles(speedPercentiles)[SAFE_OFFSET(0)]
            )
          ) AS INT64),
          CAST(
            tomtom.arrayMetersToMiles(speedPercentiles)[SAFE_OFFSET(18)]
            + 1.2 * (
              tomtom.arrayMetersToMiles(speedPercentiles)[SAFE_OFFSET(18)]
              - tomtom.arrayMetersToMiles(speedPercentiles)[SAFE_OFFSET(17)]
            ) AS INT64)
        ),
        GREATEST(CAST(ROUND(sampleSize) AS INT64), 1)
      ),
      []           -- ← empty array when slice has no percentiles
    ) AS synthetic_speeds
  FROM source
),

agg AS (
  SELECT
    dsegId,
    geohash,
    hour_ts                                AS dateHour,

    SUM(sampleSize)                        AS total_sampleSize,

    SAFE_DIVIDE(SUM(sampleSize * averageSpeedMetersPerHour),
                SUM(sampleSize))           AS avgSpeedMetersPerHour,
    SAFE_DIVIDE(SUM(sampleSize * harmonicAverageSpeedMetersPerHour),
                SUM(sampleSize))           AS harmSpeedMetersPerHour,
    SAFE_DIVIDE(SUM(sampleSize * medianSpeedMetersPerHour),
                SUM(sampleSize))           AS medianSpeedMetersPerHour,
    SAFE_DIVIDE(SUM(sampleSize * standardDeviationSpeedMetersPerHour),
                SUM(sampleSize))           AS stdSpeedMetersPerHour,

    ARRAY_CONCAT_AGG(synthetic_speeds)     AS all_speeds_mph     -- may be []
  FROM simulated
  GROUP BY dsegId, geohash, hour_ts
)

SELECT
  dsegId,
  geohash,
  dateHour,
  total_sampleSize,
  avgSpeedMetersPerHour,
  harmSpeedMetersPerHour,
  medianSpeedMetersPerHour,
  stdSpeedMetersPerHour,

  -- ---------- hour-level 19-bin percentile array ----------
  (
    CASE
      WHEN ARRAY_LENGTH(all_speeds_mph) = 0 THEN []            -- no valid slices
      ELSE (SELECT APPROX_QUANTILES(val, 19)
            FROM UNNEST(all_speeds_mph) AS val)
    END
  ) AS speedPercentiles_mph
FROM agg;



=========

2.1 Compare total sample size

-- Expect zero rows ⇒ counts match
WITH slices AS (
  SELECT dsegId, geohash,
         TIMESTAMP_TRUNC(dateHour, HOUR) AS hr,
         SUM(sampleSize) AS sum_samples
  FROM `stl-datascience.tomtom.tt_bulk_test_geohash6`
  TABLESAMPLE SYSTEM (0.1 PERCENT)
  GROUP BY dsegId, geohash, hr
),
hourly AS (
  SELECT dsegId, geohash, dateHour AS hr, total_sampleSize
  FROM `stl-datascience.tomtom.tt_bulk_test_geohash6_hourly`
)
SELECT *
FROM hourly
JOIN slices USING (dsegId, geohash, hr)
WHERE total_sampleSize <> sum_samples
LIMIT 10;


============

2.2 Compare weighted average speed (m/h)

-- Should return zero rows (or tiny float wiggle room)
WITH slices AS (
  SELECT dsegId, geohash,
         TIMESTAMP_TRUNC(dateHour, HOUR) AS hr,
         SAFE_DIVIDE(SUM(sampleSize * averageSpeedMetersPerHour),
                     SUM(sampleSize))    AS avg_speed_direct
  FROM `stl-datascience.tomtom.tt_bulk_test_geohash6`
  TABLESAMPLE SYSTEM (0.1 PERCENT)
  GROUP BY dsegId, geohash, hr
),
hourly AS (
  SELECT dsegId, geohash, dateHour AS hr, avgSpeedMetersPerHour
  FROM `stl-datascience.tomtom.tt_bulk_test_geohash6_hourly`
)
SELECT *
FROM hourly
JOIN slices USING (dsegId, geohash, hr)
WHERE ABS(avgSpeedMetersPerHour - avg_speed_direct) > 1e-6   -- tolerance
LIMIT 10;

==============

2.3 Spot-check median (p50) percentiles

WITH direct_p50 AS (
  SELECT dsegId, geohash,
         TIMESTAMP_TRUNC(dateHour, HOUR) AS hr,
         APPROX_QUANTILES(
           tomtom.arrayMetersToMiles(speedPercentiles), 19
         )[SAFE_OFFSET(9)]              AS p50_direct
  FROM `stl-datascience.tomtom.tt_bulk_test_geohash6`
  TABLESAMPLE SYSTEM (0.1 PERCENT)
  WHERE sampleSize IS NOT NULL
    AND ARRAY_LENGTH(speedPercentiles)=19
  GROUP BY dsegId, geohash, hr
),
hourly AS (
  SELECT dsegId, geohash, dateHour AS hr,
         speedPercentiles_mph[SAFE_OFFSET(9)] AS p50_hour
  FROM `stl-datascience.tomtom.tt_bulk_test_geohash6_hourly`
)
SELECT *
FROM hourly
JOIN direct_p50 USING (dsegId, geohash, hr)
-- show rows where the two p50s differ by > 1 mph
WHERE ABS(p50_hour - p50_direct) > 1
LIMIT 10;
