-- ─────────────────────────────────────────────────────────────
--  Hour-level aggregation from 15-minute table (geo6)
--  • Legacy TomTom percentile logic
--  • Units in metres / hour everywhere
--  • Keeps hour row even when all four slices lack percentiles
--  • Toggle the hash-sample predicate for cheap QA runs
-- ─────────────────────────────────────────────────────────────
CREATE OR REPLACE TABLE
  `stl-datascience.tomtom.tt_bulk_test_geohash6_hourly`
PARTITION BY TIMESTAMP_TRUNC(dateHour, MONTH)
CLUSTER BY geohash, dsegId, dateHour AS

WITH slices AS (
  SELECT
    dsegId,
    geohash,
    TIMESTAMP_TRUNC(dateHour, HOUR)                        AS hour_ts,
    sampleSize                                             AS n,
    averageSpeedMetersPerHour                              AS v_arith,
    harmonicAverageSpeedMetersPerHour                      AS v_harm,
    -- comment-out next WHERE block for full build; keep for 0.1 % QA
    -- WHERE MOD(ABS(FARM_FINGERPRINT(
    --         CONCAT(CAST(dsegId AS STRING),'#',geohash,'#',
    --                FORMAT_TIMESTAMP('%F %H:%M',dateHour))
    --       )),1000)=0                                      -- ≈0.1 %
    tomtom.arrayMetersToMiles(speedPercentiles)            AS sp_mph
  FROM `stl-datascience.tomtom.tt_bulk_test_geohash6`
),

simulated AS (
  SELECT
    dsegId,
    geohash,
    hour_ts,
    n,
    v_arith,
    v_harm,

    -- synthetic sample array (mph); empty if slice lacks percentiles
    IF(
      ARRAY_LENGTH(sp_mph) = 19,
      tomtom.resampleSpeedValues(
        tomtom.midpointMethodInt(
          sp_mph,
          /* extrapolated min & max in mph */
          CAST(GREATEST(
            0,
            sp_mph[OFFSET(0)] -
            1.2 * (sp_mph[OFFSET(1)] - sp_mph[OFFSET(0)])
          ) AS INT64),
          CAST(
            sp_mph[OFFSET(18)] +
            1.2 * (sp_mph[OFFSET(18)] - sp_mph[OFFSET(17)])
          AS INT64)
        ),
        GREATEST(CAST(ROUND(n) AS INT64), 1)
      ),
      []      -- no distribution written by TomTom for this slice
    ) AS sims_mph
  FROM slices
),

agg AS (
  SELECT
    dsegId,
    geohash,
    hour_ts                                           AS dateHour,

    /* ---- numeric aggregates ---- */
    SUM(n)                                            AS total_sampleSize,

    SAFE_DIVIDE(SUM(n * v_arith), SUM(n))             AS avgSpeedMetersPerHour,

    SAFE_DIVIDE(SUM(n), SUM(n / NULLIF(v_harm,0)))    AS harmSpeedMetersPerHour,

    /* concatenate all synthetic samples (may be []) */
    ARRAY_CONCAT_AGG(sims_mph)                        AS all_sims_mph
  FROM simulated
  GROUP BY dsegId, geohash, hour_ts
)

SELECT
  dsegId,
  geohash,
  dateHour,
  total_sampleSize,
  avgSpeedMetersPerHour,
  harmSpeedMetersPerHour,

  /* ---- 19-bin percentile array in metres / hour ---- */
  CASE
    WHEN ARRAY_LENGTH(all_sims_mph) = 0 THEN []
    ELSE (
      SELECT APPROX_QUANTILES(val * 1609.34, 19)      -- mph → m / h
      FROM UNNEST(all_sims_mph) AS val
    )
  END AS speedPercentiles
FROM agg;
