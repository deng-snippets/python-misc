-- ------------------------------------------------------------
-- DRY‑RUN / MINI‑LOAD: long → wide with struct‑wrapped arrays
-- ------------------------------------------------------------

-- ------------------------------------------------------------
-- 1) Re‑create wide table (struct‑wrapped arrays ⇒ no NULL elem error)
-- ------------------------------------------------------------
CREATE OR REPLACE TABLE `stl-datascience.tomtom.tt_bulk_test_geohash6_wide` (
  geohash                STRING,
  dsegId                 INT64,
  dateHour               TIMESTAMP,

  sampleSize_15m         ARRAY<STRUCT<val INT64>>,
  avgSpeed_15m           ARRAY<STRUCT<val INT64>>,
  harmSpeed_15m          ARRAY<STRUCT<val INT64>>,
  medianSpeed_15m        ARRAY<STRUCT<val INT64>>,
  stdSpeed_15m           ARRAY<STRUCT<val INT64>>,

  speedPercentiles_15m   ARRAY<STRUCT<percentiles ARRAY<INT64>>>,

  total_sampleSize       INT64
)
PARTITION BY DATE_TRUNC(dateHour, MONTH)
CLUSTER BY geohash, dsegId, dateHour;

-- ------------------------------------------------------------
-- 2) Mini‑load: reference the source table **only once** with
--    TABLESAMPLE to avoid the “sampling not supported” error.
-- ------------------------------------------------------------
INSERT INTO `stl-datascience.tomtom.tt_bulk_test_geohash6_wide`
WITH sample_rows AS (
  SELECT
    geohash,
    dsegId,
    TIMESTAMP_TRUNC(dateHour, HOUR)                 AS dateHour,
    DIV(EXTRACT(MINUTE FROM dateHour), 15)          AS bin_idx,   -- 0‑3
    sampleSize,
    averageSpeedMetersPerHour,
    harmonicAverageSpeedMetersPerHour,
    medianSpeedMetersPerHour,
    standardDeviationSpeedMetersPerHour,
    speedPercentiles
  FROM `stl-datascience.tomtom.tt_bulk_test_geohash6`
  TABLESAMPLE SYSTEM (0.2 PERCENT)                 -- one reference only
),
by_hour AS (
  SELECT
    geohash,
    dsegId,
    dateHour,

    [STRUCT(MAX(IF(bin_idx = 0, sampleSize, NULL)) AS val),
     STRUCT(MAX(IF(bin_idx = 1, sampleSize, NULL)) AS val),
     STRUCT(MAX(IF(bin_idx = 2, sampleSize, NULL)) AS val),
     STRUCT(MAX(IF(bin_idx = 3, sampleSize, NULL)) AS val)]            AS sampleSize_15m,

    [STRUCT(MAX(IF(bin_idx = 0, averageSpeedMetersPerHour, NULL)) AS val),
     STRUCT(MAX(IF(bin_idx = 1, averageSpeedMetersPerHour, NULL)) AS val),
     STRUCT(MAX(IF(bin_idx = 2, averageSpeedMetersPerHour, NULL)) AS val),
     STRUCT(MAX(IF(bin_idx = 3, averageSpeedMetersPerHour, NULL)) AS val)] AS avgSpeed_15m,

    [STRUCT(MAX(IF(bin_idx = 0, harmonicAverageSpeedMetersPerHour, NULL)) AS val),
     STRUCT(MAX(IF(bin_idx = 1, harmonicAverageSpeedMetersPerHour, NULL)) AS val),
     STRUCT(MAX(IF(bin_idx = 2, harmonicAverageSpeedMetersPerHour, NULL)) AS val),
     STRUCT(MAX(IF(bin_idx = 3, harmonicAverageSpeedMetersPerHour, NULL)) AS val)] AS harmSpeed_15m,

    [STRUCT(MAX(IF(bin_idx = 0, medianSpeedMetersPerHour, NULL)) AS val),
     STRUCT(MAX(IF(bin_idx = 1, medianSpeedMetersPerHour, NULL)) AS val),
     STRUCT(MAX(IF(bin_idx = 2, medianSpeedMetersPerHour, NULL)) AS val),
     STRUCT(MAX(IF(bin_idx = 3, medianSpeedMetersPerHour, NULL)) AS val)]             AS medianSpeed_15m,

    [STRUCT(MAX(IF(bin_idx = 0, standardDeviationSpeedMetersPerHour, NULL)) AS val),
     STRUCT(MAX(IF(bin_idx = 1, standardDeviationSpeedMetersPerHour, NULL)) AS val),
     STRUCT(MAX(IF(bin_idx = 2, standardDeviationSpeedMetersPerHour, NULL)) AS val),
     STRUCT(MAX(IF(bin_idx = 3, standardDeviationSpeedMetersPerHour, NULL)) AS val)]  AS stdSpeed_15m,

    [STRUCT(ANY_VALUE(IF(bin_idx = 0, speedPercentiles, NULL)) AS percentiles),
     STRUCT(ANY_VALUE(IF(bin_idx = 1, speedPercentiles, NULL)) AS percentiles),
     STRUCT(ANY_VALUE(IF(bin_idx = 2, speedPercentiles, NULL)) AS percentiles),
     STRUCT(ANY_VALUE(IF(bin_idx = 3, speedPercentiles, NULL)) AS percentiles)]       AS speedPercentiles_15m,

    SUM(sampleSize) AS total_sampleSize
  FROM sample_rows
  GROUP BY geohash, dsegId, dateHour
)
SELECT * FROM by_hour;

-- ------------------------------------------------------------
-- Result: a few hundred rows written, tiny cost.  Rerun QA.
-- To scale up, swap TABLESAMPLE for a RAND() filter or simply
-- remove the sample_rows CTE and point at the full table.
-- ------------------------------------------------------------
